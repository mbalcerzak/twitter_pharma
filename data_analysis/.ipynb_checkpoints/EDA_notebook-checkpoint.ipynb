{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pharma NLP tweets analysis\n",
    "\n",
    "\n",
    "Having found the Harvard Business Review's ranking of the most skilled companies on twitter [\"50 Companies That Get Twitter – and 50 That Don’t\"](https://hbr.org/2015/04/the-best-and-worst-corporate-tweeters), I decided to carry out a small analysis myself. \n",
    "\n",
    "The HBR's analysis was conducted on 350,00 tweets of 300 companies listed on NASDAQ, NYSE or FTSE and presents an \"empathy\" scoring, where the most empathetic companies are on top. Even though the author explains that the methodology assumes empathy consists of: \"reassurance, authenticity, and emotional connection\" it's a difficult task to actually measure it in real life. On the other hand, it is possible to measure engagement.\n",
    "\n",
    "\n",
    "In the ranking AstraZeneca took the last place - why is that? Do we differ that much from other pharmaceutical companies? Let's check!\n",
    "\n",
    "Code can be found on my GitHub account: [mbalcerzak](https://github.com/mbalcerzak/twitter_pharma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do pharmaceutical companies need Twitter?\n",
    "##### It's important to engage with patients on twitter \n",
    "\n",
    "The new generation get their news from social media. Twitter is a way to communicate and educate patients, solve problems and inform. That's why it's so important to engage with the followers in an authentic and empathetic way. \n",
    "\n",
    "According to the article [\"It’s Time to Tweet—How Pharma Should Be Using Twitter\"](https://www.pm360online.com/its-time-to-tweet-how-pharma-should-be-using-twitter/) there are numerous benefits to engaging with patients and investors on social media:\n",
    " - top 10 biggest pharmaceutical companies already use Twitter\n",
    " - accessible and timely information for patients and regulators\n",
    " - interacting with opinion leaders and \"pharmaceutical influencers\"\n",
    " - increased comapnu reputation\n",
    " - better customer service\n",
    " - advertising opportunity (also for future hiring)\n",
    "\n",
    "\n",
    "##### Good text source for NLP analysis\n",
    "\n",
    "Personally, I wanted to learn web scraping and Natural Language Processing and Twitter provides excellent starting tools and is a great data mining playground. Twitter encourages people to have open discussions and is frequently used by both companies and consumers. Most businesses use twitter, pharmaceutical companies are no exception. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping data from Twitter\n",
    "\n",
    "Tweets of the chosen companies were collected using [**tweepy**](https://www.tweepy.org/) package for Python 3.x. The accounts are officially verified. \n",
    "- [AstraZeneca](https://twitter.com/AstraZeneca) \n",
    "- [Johnson & Johnson](https://twitter.com/JNJCares)\n",
    "- [Roche](https://twitter.com/Roche)\n",
    "- [Pfizer](https://twitter.com/Pfizer) \n",
    "- [Novartis](https://twitter.com/Novartis)\n",
    "- [BayerPharma](https://twitter.com/BayerPharma) \n",
    "- [Merck](https://twitter.com/Merck) \n",
    "- [GSK](https://twitter.com/GSK) \n",
    "- [Sanofi](https://twitter.com/Sanofi)\n",
    "- [Abbvie](https://twitter.com/abbvie)\n",
    "- [Abbott](https://twitter.com/AbbottGlobal) \n",
    "- [Eli Lilly and Company](https://twitter.com/LillyPad) \n",
    "- [Amgen](https://twitter.com/Amgen) \n",
    "- [Bristol-Myers Squibb](https://twitter.com/bmsnews) \n",
    "- [GileadSciences](https://twitter.com/GileadSciences) \n",
    "\n",
    "My code is fully available on GitHub: [mbalcerzak/../webscraping/](https://github.com/mbalcerzak/twitter_pharma/tree/master/web_scraping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages used\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import re\n",
    "import os\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy import stats\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/malgo_000/Desktop/Web_scraping/twitter_scraping/tweet_texts_pharma/'\n",
    "#path = os.path.join(os.getcwd(), 'tweet_texts_pharma/')\n",
    "\n",
    "def prepare_dataset(company):\n",
    "    df = pd.read_csv(path + '%s_tweets.txt' % company, sep='|')\n",
    "    \n",
    "    df['company'] = company\n",
    "    df['id'] = df['id'].apply(str)\n",
    "    \n",
    "    df['hashtags'] = df['text'].apply(lambda s: re.findall(r'#(\\w+)', s))\n",
    "    df['num_hash'] = df['hashtags'].apply(len)\n",
    "    \n",
    "    df['tagged'] = df['text'].apply(lambda s: re.findall(r'@(\\w+)', s))\n",
    "    \n",
    "    def clean_tweet(tweet):\n",
    "        check = '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)'\n",
    "        return ' '.join(re.sub(check, ' ', tweet).split()).replace('RT ','')\n",
    "        \n",
    "    df['clean_tweet'] = [clean_tweet(tweet) for tweet in df['text']]    \n",
    "    df['len'] = df['clean_tweet'].apply(len)\n",
    "    \n",
    "    df['datetime'] = pd.to_datetime(df['created_at'])\n",
    "    df['hour'] = df['datetime'].apply(lambda x: x.hour)\n",
    "    df['month'] = df['datetime'].apply(lambda x: x.month)\n",
    "    df['day'] = df['datetime'].apply(lambda x: x.day)\n",
    "    df['year'] = df['datetime'].apply(lambda x: x.year)\n",
    "    df = df.drop(columns=['created_at'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = prepare_dataset('AstraZeneca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "**id** -  \n",
    "**text** -  \n",
    "**retweet** -  \n",
    "**source** -  \n",
    "**fav** -  \n",
    "**RT** -  \n",
    "**hashtags** -  \n",
    "**company** -  \n",
    "**num_hash** -  \n",
    "**tagged** -  \n",
    "**clean_tweet** -  \n",
    "**len** -  \n",
    "**datetime** -  \n",
    "**hour** -  \n",
    "**month** -  \n",
    "**day** -  \n",
    "**year** -  \n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "    def clean_tweet(tweet):\n",
    "        check = '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)'\n",
    "        return ' '.join(re.sub(check, ' ', tweet).split()).replace('RT ','')\n",
    "```        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**id** -  \n",
      "**text** -  \n",
      "**retweet** -  \n",
      "**source** -  \n",
      "**fav** -  \n",
      "**RT** -  \n",
      "**hashtags** -  \n",
      "**company** -  \n",
      "**num_hash** -  \n",
      "**tagged** -  \n",
      "**clean_tweet** -  \n",
      "**len** -  \n",
      "**datetime** -  \n",
      "**hour** -  \n",
      "**month** -  \n",
      "**day** -  \n",
      "**year** -  \n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(['**'+x+'** -  ' for x in list(df)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# getting rid of outliers\n",
    "df['z'] = np.abs(stats.zscore(df['fav']))\n",
    "print('Tweets that can be classified as outliers when it comes to the number of likes: ')\n",
    "display(df[df['z'] >= 3])\n",
    "df = df[df['z'] < 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of AstraZeneca tweets: 142\n",
      "Number of likes for the most liked tweet: 1213\n",
      "An average tweet received 16 likes\n",
      "AZ tweets got on average 12 likes and retweets 31\n"
     ]
    }
   ],
   "source": [
    "print('Average length of AstraZeneca tweets: {}'.format(round(np.mean(df['len']))))\n",
    "print('Number of likes for the most liked tweet: {}'.format(np.max(df['fav'])))\n",
    "print('An average tweet received {} likes'.format(round(np.mean(df['fav']))))\n",
    "print('AZ tweets got on average {} likes and retweets {}'.format(round(np.mean(df['fav'][df['retweet']==False])),\n",
    "                                                                 round(np.mean(df['fav'][df['retweet']==True]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tweet that received the most likes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>fav</th>\n",
       "      <th>RT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>Happy LGBTSTEMDay one and all Let s work together for more inclusivity and more support for LGBTQ people in STEM</td>\n",
       "      <td>[LGBTSTEMDay]</td>\n",
       "      <td>1213</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                          clean_tweet  \\\n",
       "956  Happy LGBTSTEMDay one and all Let s work together for more inclusivity and more support for LGBTQ people in STEM   \n",
       "\n",
       "          hashtags   fav   RT  \n",
       "956  [LGBTSTEMDay]  1213  790  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most liked non-retweeted tweet:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>fav</th>\n",
       "      <th>RT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>We are named a global sustainability leader by the 2018 Dow Jones Sustainability Index We received industry top scores in the areas Environmental Reporting Labour Practice Indicators and Health Outcome Contribution sustainability DJSI ESG</td>\n",
       "      <td>[sustainability, DJSI, ESG]</td>\n",
       "      <td>325</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                        clean_tweet  \\\n",
       "845  We are named a global sustainability leader by the 2018 Dow Jones Sustainability Index We received industry top scores in the areas Environmental Reporting Labour Practice Indicators and Health Outcome Contribution sustainability DJSI ESG   \n",
       "\n",
       "                        hashtags  fav  RT  \n",
       "845  [sustainability, DJSI, ESG]  325  16  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('A tweet that received the most likes:')\n",
    "display(df[['clean_tweet','hashtags','fav','RT']][df['fav']==max(df['fav'])])\n",
    "\n",
    "print('Most liked non-retweeted tweet:')\n",
    "df_org = df[df['retweet']==False]\n",
    "display(df_org[['clean_tweet','hashtags','fav','RT']][df_org['fav']==max(df_org['fav'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text normalisation\n",
    "\n",
    "- converting text into lower-case words\n",
    "- removing hashtags, links and stopwords\n",
    "- lemmatization [Wikipedia explanation](https://en.wikipedia.org/wiki/Lemmatisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all datasets with company tweets into one dataset\n",
    "\n",
    "def combine_tweets(company_names):\n",
    "    df_all = df\n",
    "    for company in company_names:\n",
    "        df_all = df_all.append(prepare_dataset(company), ignore_index = True)        \n",
    "    return df_all\n",
    "\n",
    "df_all = combine_tweets(['JNJCares', 'Roche', 'Pfizer','Novartis', \n",
    "                         'BayerPharma', 'Merck','GSK','Sanofi', 'abbvie', \n",
    "                         'AbbottGlobal','LillyPad', 'Amgen', 'bmsnews',\n",
    "                         'GileadSciences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# I'm setting up the list of words used frequently in order to remove them later from tweets\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#print(sorted(stop_words))\n",
    "\n",
    "\n",
    "\n",
    "## Time series\n",
    "#time_fav = pd.Series(data=df['fav'].values, index=df['created_at'])\n",
    "#time_fav.plot(figsize=(16, 4), color = 'r', label = 'favourites', legend = True)                \n",
    "#\n",
    "## for retweets\n",
    "#time_rt = pd.Series(data=df['RT'].values, index=df['created_at'])\n",
    "#time_rt.plot(figsize=(16, 4), color = 'b', label = 'retweets', legend = True)\n",
    "#\n",
    "## number of hashtags\n",
    "#time_rt = pd.Series(data=df['num_hash'].values, index=df['created_at'])\n",
    "#time_rt.plot(figsize=(16, 4), color = 'g', label = 'hashtags', legend = True)\n",
    "#\n",
    "#plt.show()   \n",
    "\n",
    "# barplot of number of hashtags per tweet\n",
    " \n",
    "df['num_hash'].hist(color = 'b', label = 'numer of hashtags')\n",
    "plt.show()  \n",
    "\n",
    "counter_hsh = collections.Counter(df['num_hash'])\n",
    "print(counter_hsh.most_common()) \n",
    "\n",
    "#lemmatize + lower()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "# join all words:\n",
    "all_tweets = []\n",
    "all_tweets_lem = []\n",
    "\n",
    "for tweet in df['clean_tweet']:\n",
    "    for word in tweet.split(' '):\n",
    "        if word.lower() not in stop_words:\n",
    "            all_tweets.append(word.lower())\n",
    "            all_tweets_lem.append(lemmatizer.lemmatize(word.lower()))\n",
    "            \n",
    "# most common words\n",
    "counter = collections.Counter(all_tweets)\n",
    "print(counter.most_common(15))         \n",
    "\n",
    "# most common lemmatized words\n",
    "counter_l = collections.Counter(all_tweets_lem)\n",
    "print(counter_l.most_common(15))\n",
    "\n",
    "# most liked tweets od AZ\n",
    "df.nlargest(3, 'fav')\n",
    "df_all.nlargest(15, 'fav')['clean_tweet']# overall\n",
    "\n",
    "d = pd.DataFrame(counter.most_common(15), columns = ['Word', 'Count'])\n",
    "d.plot.bar(x='Word',y='Count')\n",
    "\n",
    "# worcloud\n",
    "#plt.figure(figsize = (30,30))\n",
    "#wordcloud_ = WordCloud(\n",
    "#                      background_color = 'white',\n",
    "#                      max_words = 1000,\n",
    "#                      max_font_size = 120,\n",
    "#                      width=600, height=400,\n",
    "#                      random_state = 42\n",
    "#                    ).generate(' '.join([a for a in all_tweets]))\n",
    "#\n",
    "##Plotting the word cloud\n",
    "#plt.imshow(wordcloud_)\n",
    "#plt.axis('off')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "# most common hashtags\n",
    "hsh_list= []\n",
    "for h in list(df['hashtags']):\n",
    "    hsh_list += h \n",
    "      \n",
    "counter_h = collections.Counter(hsh_list)\n",
    "print(counter_h.most_common(15)) \n",
    "\n",
    "# wordcloud of hashtags\n",
    "#plt.figure(figsize = (30,30))\n",
    "#wordcloud_ = WordCloud(\n",
    "#                      background_color = 'white',\n",
    "#                      max_words = 1000,\n",
    "#                      max_font_size = 120,\n",
    "#                      width=800 ,height=400,\n",
    "#                      random_state = 42,\n",
    "#                      collocations=False,\n",
    "#                    ).generate(' '.join([a for a in hsh_list]))\n",
    "#\n",
    "#plt.imshow(wordcloud_)\n",
    "#plt.axis('off')\n",
    "#plt.show()\n",
    "\n",
    "# most popular tagged accounts\n",
    "tag_list= []\n",
    "for t in list(df['tagged']):\n",
    "    tag_list += t \n",
    "      \n",
    "counter_t = collections.Counter(tag_list)\n",
    "print(counter_t.most_common(15))   \n",
    "\n",
    "\n",
    "# number of retweets\n",
    "\n",
    "#ax1 = sns.countplot(df['retweet'], palette='rainbow')\n",
    "##ax1.set_title('%s's tweets' % company)\n",
    "#ax1.set(xticklabels=['Tweets','Retweets'])\n",
    "\n",
    "#Number of tweets hourly\n",
    "#hourly_tweets = df['hour'].size().unstack()\n",
    "#hourly_tweets.plot(title='Hourly Tweet Counts', colormap='coolwarm')\n",
    "\n",
    "hourly_tweets = df_all.groupby(['hour', 'company']).size().unstack()\n",
    "hourly_tweets.plot(title='Hourly Tweet Counts', stacked = True, colormap='coolwarm')\n",
    "\n",
    "#Number of tweets by the months\n",
    "monthly_tweets = df_all.groupby(['month', 'company']).size().unstack()\n",
    "monthly_tweets.plot(title='Monthly Tweet Counts', colormap='winter')\n",
    "\n",
    "# scatterplot of likes vs hour of posting\n",
    "\n",
    "ax = sns.scatterplot(x=\"hour\", y=\"fav\", hue=\"company\", data=df_all, palette=\"Purples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering (themes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sector averages\n",
    "\n",
    "- how many times they tweeter per period\n",
    "- 3 most popular posts of this year, which company\n",
    "- tweets with / without images\n",
    "\n",
    "- best time to tweet\n",
    "\"Human working memory exhibits inherent variation across time of day and is highest when we wake up in the morning, lowest in mid-afternoon, and moderate in the evening. Higher availability of working memory makes individuals alert and feel the need to seek information. This means that consumers’ desire to engage with content will likely be highest in the morning, lowest in the afternoon, and moderate in the evening.\"\n",
    "\"Assuming the majority of the audience start their day in the morning, it is ideal to post content conveying high-arousal emotion (i.e., angry or worried) in the morning and “deep think” content in the afternoon\"\n",
    "https://hbr.org/2018/09/a-study-shows-the-best-times-of-day-to-post-to-social-media\n",
    "\n",
    "- długość tweeta\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "1. [\"50 Companies That Get Twitter – and 50 That Don’t\"](https://hbr.org/2015/04/the-best-and-worst-corporate-tweeters)\n",
    "2. [List of largest pharmaceutical companies by revenue](https://en.wikipedia.org/wiki/List_of_largest_pharmaceutical_companies_by_revenue)\n",
    "3. [Twitter Dev Documentation](https://developer.twitter.com/en/docs)\n",
    "4. [\"It’s Time to Tweet—How Pharma Should Be Using Twitter\"](https://www.pm360online.com/its-time-to-tweet-how-pharma-should-be-using-twitter/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
