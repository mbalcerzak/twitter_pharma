{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pharma NLP tweets analysis\n",
    "\n",
    "\n",
    "Having found the Harvard Business Review's ranking of the most skilled companies on twitter [\"50 Companies That Get Twitter – and 50 That Don’t\"](https://hbr.org/2015/04/the-best-and-worst-corporate-tweeters), I decided to carry out a small analysis myself. \n",
    "\n",
    "The HBR's analysis was conducted on 350,00 tweets of 300 companies listed on NASDAQ, NYSE or FTSE and presents an \"empathy\" scoring, where the most empathetic companies are on top. Even though the author explains that the methodology assumes empathy consists of: \"reassurance, authenticity, and emotional connection\" it's a difficult task to actually measure it in real life. On the other hand, it is possible to measure engagement.\n",
    "\n",
    "\n",
    "In the ranking AstraZeneca took the last place - why is that? Do we differ that much from other pharmaceutical companies? Let's check!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do pharmaceutical companies need Twitter?\n",
    "##### It's important to engage with patients on twitter \n",
    "\n",
    "The new generation get their news from social media. Twitter is a way to communicate and educate patients, solve problems and inform. That's why it's so important to engage with the followers in an authentic and empathetic way. \n",
    "\n",
    "According to the article [\"It’s Time to Tweet—How Pharma Should Be Using Twitter\"](https://www.pm360online.com/its-time-to-tweet-how-pharma-should-be-using-twitter/) there are numerous benefits to engaging with patients and investors on social media:\n",
    " - top 10 biggest pharmaceutical companies already use Twitter\n",
    " - accessible and timely information for patients and regulators\n",
    " - interacting with opinion leaders and \"pharmaceutical influencers\"\n",
    " - increased comapnu reputation\n",
    " - better customer service\n",
    " - advertising opportunity (also for future hiring)\n",
    "\n",
    "\n",
    "##### Good text source for NLP analysis\n",
    "\n",
    "Personally, I wanted to learn web scraping and Natural Language Processing and Twitter provides excellent starting tools and is a great data mining playground. Twitter encourages people to have open discussions and is frequently used by both companies and consumers. Most businesses use twitter, pharmaceutical companies are no exception. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text normalisation\n",
    "\n",
    "- converting text into lower-case words\n",
    "- removing hashtags, links and stopwords\n",
    "- lemmatization [Wikipedia explanation](https://en.wikipedia.org/wiki/Lemmatisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "\n",
    "- word cloud\n",
    "- map with follower's location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import re\n",
    "import os\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy import stats\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#path = 'C:/Users/malgo_000/Desktop/Web_scraping/twitter_scraping/tweet_texts_pharma/'\n",
    "path = os.path.join(os.getcwd(), 'tweet_texts_pharma/')\n",
    "\n",
    "#company = 'AstraZeneca'\n",
    "\n",
    "def prepare_dataset(company):\n",
    "    df = pd.read_csv(path + '%s_tweets.txt' % company, sep='|')\n",
    "    \n",
    "    df['company'] = company\n",
    "    df['id'] = df['id'].apply(str)\n",
    "    # cleaning tweet text\n",
    "    df['hashtags'] = df['text'].apply(lambda s: re.findall(r'#(\\w+)', s))\n",
    "    df['num_hash'] = df['hashtags'].apply(len)\n",
    "    df['retweet'] = df['text'].apply(lambda s: True if 'RT ' in s else False)\n",
    "    df['tagged'] = df['text'].apply(lambda s: re.findall(r'@(\\w+)', s))\n",
    "    \n",
    "    def clean_tweet(tweet):\n",
    "        check = '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)'\n",
    "        return ' '.join(re.sub(check, ' ', tweet).split()).replace('RT ','')\n",
    "        \n",
    "    df['clean_tweet'] = [clean_tweet(tweet) for tweet in df['text']]    \n",
    "    df['len'] = df['clean_tweet'].apply(len)\n",
    "    \n",
    "    # getting time and date\n",
    "    df['datetime'] = pd.to_datetime(df['created_at'])\n",
    "    df['hour'] = df['datetime'].apply(lambda x: x.hour)\n",
    "    df['month'] = df['datetime'].apply(lambda x: x.month)\n",
    "    df['day'] = df['datetime'].apply(lambda x: x.day)\n",
    "    df['year'] = df['datetime'].apply(lambda x: x.year)\n",
    "    \n",
    "    # getting rid of outliers\n",
    "    df['z'] = np.abs(stats.zscore(df['fav']))\n",
    "    df = df[df['z'] < 3]\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = prepare_dataset('AstraZeneca')\n",
    "\n",
    "def combine_tweets(company_names):\n",
    "    df_all = df\n",
    "    for company in company_names:\n",
    "        df_all = df_all.append(prepare_dataset(company), ignore_index = True)\n",
    "        \n",
    "        \n",
    "    return df_all\n",
    "\n",
    "df_all = combine_tweets(['JNJCares', 'Roche', 'Pfizer','Novartis', \n",
    "                         'BayerPharma', 'Merck','GSK','Sanofi', 'abbvie', \n",
    "                         'AbbottGlobal','LillyPad', 'Amgen', 'bmsnews',\n",
    "                         'GileadSciences'])\n",
    "\n",
    "df_all.head(5)\n",
    "\n",
    "# average length of all tweets for AZ\n",
    "print(round(np.mean(df['len'])))\n",
    "print(round(np.mean(df_all['len'])))\n",
    "\n",
    "# number of likes for the most liked tweet and average likes\n",
    "print(np.max(df['fav']))\n",
    "print(round(np.mean(df['fav'])))\n",
    "\n",
    "# number of likes for original posts\n",
    "print(np.max(df['fav'][df['retweet']==False]))\n",
    "print(round(np.mean(df['fav'][df['retweet']==False])))\n",
    "\n",
    "# and retweets\n",
    "print(round(np.mean(df['fav'][df['retweet']==True])))\n",
    "\n",
    "# number of retweets for the most retweeted tweet\n",
    "print(np.max(df['RT']))\n",
    "\n",
    "#getting rid of outliers\n",
    "sns.boxplot(x=df['fav'])\n",
    "#df['z'] = np.abs(stats.zscore(df['fav']))\n",
    "#df = df[df['z'] < 3]\n",
    "\n",
    "## Time series\n",
    "#time_fav = pd.Series(data=df['fav'].values, index=df['created_at'])\n",
    "#time_fav.plot(figsize=(16, 4), color = 'r', label = 'favourites', legend = True)                \n",
    "#\n",
    "## for retweets\n",
    "#time_rt = pd.Series(data=df['RT'].values, index=df['created_at'])\n",
    "#time_rt.plot(figsize=(16, 4), color = 'b', label = 'retweets', legend = True)\n",
    "#\n",
    "## number of hashtags\n",
    "#time_rt = pd.Series(data=df['num_hash'].values, index=df['created_at'])\n",
    "#time_rt.plot(figsize=(16, 4), color = 'g', label = 'hashtags', legend = True)\n",
    "#\n",
    "#plt.show()   \n",
    "\n",
    "# barplot of number of hashtags per tweet\n",
    " \n",
    "df['num_hash'].hist(color = 'b', label = 'numer of hashtags')\n",
    "plt.show()  \n",
    "\n",
    "counter_hsh = collections.Counter(df['num_hash'])\n",
    "print(counter_hsh.most_common()) \n",
    "\n",
    "#lemmatize + lower()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "# join all words:\n",
    "all_tweets = []\n",
    "all_tweets_lem = []\n",
    "\n",
    "for tweet in df['clean_tweet']:\n",
    "    for word in tweet.split(' '):\n",
    "        if word.lower() not in stop_words:\n",
    "            all_tweets.append(word.lower())\n",
    "            all_tweets_lem.append(lemmatizer.lemmatize(word.lower()))\n",
    "            \n",
    "# most common words\n",
    "counter = collections.Counter(all_tweets)\n",
    "print(counter.most_common(15))         \n",
    "\n",
    "# most common lemmatized words\n",
    "counter_l = collections.Counter(all_tweets_lem)\n",
    "print(counter_l.most_common(15))\n",
    "\n",
    "# most liked tweets od AZ\n",
    "df.nlargest(3, 'fav')\n",
    "df_all.nlargest(15, 'fav')['clean_tweet']# overall\n",
    "\n",
    "d = pd.DataFrame(counter.most_common(15), columns = ['Word', 'Count'])\n",
    "d.plot.bar(x='Word',y='Count')\n",
    "\n",
    "# worcloud\n",
    "#plt.figure(figsize = (30,30))\n",
    "#wordcloud_ = WordCloud(\n",
    "#                      background_color = 'white',\n",
    "#                      max_words = 1000,\n",
    "#                      max_font_size = 120,\n",
    "#                      width=600, height=400,\n",
    "#                      random_state = 42\n",
    "#                    ).generate(' '.join([a for a in all_tweets]))\n",
    "#\n",
    "##Plotting the word cloud\n",
    "#plt.imshow(wordcloud_)\n",
    "#plt.axis('off')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "# most common hashtags\n",
    "hsh_list= []\n",
    "for h in list(df['hashtags']):\n",
    "    hsh_list += h \n",
    "      \n",
    "counter_h = collections.Counter(hsh_list)\n",
    "print(counter_h.most_common(15)) \n",
    "\n",
    "# wordcloud of hashtags\n",
    "#plt.figure(figsize = (30,30))\n",
    "#wordcloud_ = WordCloud(\n",
    "#                      background_color = 'white',\n",
    "#                      max_words = 1000,\n",
    "#                      max_font_size = 120,\n",
    "#                      width=800 ,height=400,\n",
    "#                      random_state = 42,\n",
    "#                      collocations=False,\n",
    "#                    ).generate(' '.join([a for a in hsh_list]))\n",
    "#\n",
    "#plt.imshow(wordcloud_)\n",
    "#plt.axis('off')\n",
    "#plt.show()\n",
    "\n",
    "# most popular tagged accounts\n",
    "tag_list= []\n",
    "for t in list(df['tagged']):\n",
    "    tag_list += t \n",
    "      \n",
    "counter_t = collections.Counter(tag_list)\n",
    "print(counter_t.most_common(15))   \n",
    "\n",
    "\n",
    "# number of retweets\n",
    "\n",
    "#ax1 = sns.countplot(df['retweet'], palette='rainbow')\n",
    "##ax1.set_title('%s's tweets' % company)\n",
    "#ax1.set(xticklabels=['Tweets','Retweets'])\n",
    "\n",
    "#Number of tweets hourly\n",
    "#hourly_tweets = df['hour'].size().unstack()\n",
    "#hourly_tweets.plot(title='Hourly Tweet Counts', colormap='coolwarm')\n",
    "\n",
    "hourly_tweets = df_all.groupby(['hour', 'company']).size().unstack()\n",
    "hourly_tweets.plot(title='Hourly Tweet Counts', stacked = True, colormap='coolwarm')\n",
    "\n",
    "#Number of tweets by the months\n",
    "monthly_tweets = df_all.groupby(['month', 'company']).size().unstack()\n",
    "monthly_tweets.plot(title='Monthly Tweet Counts', colormap='winter')\n",
    "\n",
    "# scatterplot of likes vs hour of posting\n",
    "\n",
    "ax = sns.scatterplot(x=\"hour\", y=\"fav\", hue=\"company\", data=df_all, palette=\"Purples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering (themes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sector averages\n",
    "\n",
    "- how many times they tweeter per period\n",
    "- 3 most popular posts of this year, which company\n",
    "- tweets with / without images\n",
    "\n",
    "- best time to tweet\n",
    "\"Human working memory exhibits inherent variation across time of day and is highest when we wake up in the morning, lowest in mid-afternoon, and moderate in the evening. Higher availability of working memory makes individuals alert and feel the need to seek information. This means that consumers’ desire to engage with content will likely be highest in the morning, lowest in the afternoon, and moderate in the evening.\"\n",
    "\"Assuming the majority of the audience start their day in the morning, it is ideal to post content conveying high-arousal emotion (i.e., angry or worried) in the morning and “deep think” content in the afternoon\"\n",
    "https://hbr.org/2018/09/a-study-shows-the-best-times-of-day-to-post-to-social-media\n",
    "\n",
    "- długość tweeta\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "1. [\"50 Companies That Get Twitter – and 50 That Don’t\"](https://hbr.org/2015/04/the-best-and-worst-corporate-tweeters)\n",
    "2. [List of largest pharmaceutical companies by revenue](https://en.wikipedia.org/wiki/List_of_largest_pharmaceutical_companies_by_revenue)\n",
    "3. [Twitter Dev Documentation](https://developer.twitter.com/en/docs)\n",
    "4. [\"It’s Time to Tweet—How Pharma Should Be Using Twitter\"](https://www.pm360online.com/its-time-to-tweet-how-pharma-should-be-using-twitter/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
